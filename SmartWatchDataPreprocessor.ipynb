{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from PreprocessFcns import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SubjectCode</th>\n",
       "      <th>FoxInsightID</th>\n",
       "      <th>Subj ID Athena</th>\n",
       "      <th>User Name</th>\n",
       "      <th>Site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>cisuaba1</td>\n",
       "      <td>142557.0</td>\n",
       "      <td>cisuaba1</td>\n",
       "      <td>alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>cisuabb2</td>\n",
       "      <td>142558.0</td>\n",
       "      <td>cisuabb2</td>\n",
       "      <td>alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>cisuabc3</td>\n",
       "      <td>142559.0</td>\n",
       "      <td>cisuabc3</td>\n",
       "      <td>alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>cisuabd4</td>\n",
       "      <td>142560.0</td>\n",
       "      <td>cisuabd4</td>\n",
       "      <td>alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>cisnwa1</td>\n",
       "      <td>142579.0</td>\n",
       "      <td>cisnwa1</td>\n",
       "      <td>northwestern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  SubjectCode FoxInsightID  Subj ID Athena User Name  \\\n",
       "0           0       1000.0     cisuaba1        142557.0  cisuaba1   \n",
       "1           1       1001.0     cisuabb2        142558.0  cisuabb2   \n",
       "2           2       1002.0     cisuabc3        142559.0  cisuabc3   \n",
       "3           3       1003.0     cisuabd4        142560.0  cisuabd4   \n",
       "4           4       1004.0      cisnwa1        142579.0   cisnwa1   \n",
       "\n",
       "           Site  \n",
       "0       alabama  \n",
       "1       alabama  \n",
       "2       alabama  \n",
       "3       alabama  \n",
       "4  northwestern  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mot_scores = pd.read_excel('//FS2.smpp.local/RTO/CIS-PD Study/Scores/MotorTasks.xls')\n",
    "user_id_pairings = pd.read_csv(\"//FS2.smpp.local/RTO\\\\CIS-PD MUSC\\\\decoded_forms\\\\videoID.csv\")\n",
    "user_id_pairings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Features on clinical visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Subjects:\n",
    "subjDict = {1003: 'left', 1004:'right', 1005: 'left', 1007: 'left', 1009: 'right',\n",
    "           1016:'left', 1018:'left',1019:'left',1020:'right', 1023: 'right', \n",
    "           1024:'left', 1029:'left', 1030:'left',1032:'left', 1038:'left',\n",
    "           1039: 'left', 1043: 'left', 1044:'right',1046:'right', 1047: 'left',\n",
    "           1048: 'right',1049:'left', 1050: 'left', 1051:'left', 1052: 'right', \n",
    "           1053: 'left', 1054: 'left', 1055: 'right', 1056: 'left'}\n",
    "\n",
    "# # Northwestern Subjects Only:\n",
    "# subjDict = {1004:'right',1016:'left',1018:'left',1019:'left',1020:'right',1024:'left',1029:'left',1030:'left',1032:'left',\n",
    "#            1038:'left',1044:'right',1046:'right',1049:'left',1051:'left'}\n",
    "\n",
    "# Test with 1 Subject Only:\n",
    "#subjDict = {1004:'right'}\n",
    "\n",
    "\n",
    "taskList = ['Standing', 'Walking', 'Walking while counting', 'Finger to nose--right hand', \n",
    "            'Finger to nose--left hand', 'Alternating right hand movements', 'Alternating left hand movements', \n",
    "            'Sit to stand', 'Drawing on a paper', 'Typing on a computer keyboard', 'Assembling nuts and bolts', \n",
    "            'Taking a glass of water and drinking', 'Organizing sheets in a folder', 'Folding towels', 'Sitting']\n",
    "\n",
    "taskScores = ['standing','walking','walking while counting','finger to nose right','finger to nose left',\n",
    "                   'alternating right','alternating left','sit to stand','drawing on a paper',\n",
    "                   'typing on a computer keyboard','assembling nuts and bolts','taking a glass of water and drinking',\n",
    "                   'organizing sheets in a folder','folding towels','sitting']\n",
    "\n",
    "taskList_Abb = ['Stndg', 'Wlkg', 'WlkgCnt', 'FtnR', 'FtnL', 'RamR', 'RamL', 'SitStand', 'Drwg', 'Typg', 'NtsBts',\n",
    "                'Drnkg', 'Sheets', 'Fldg', 'Sitng']\n",
    "\n",
    "sessionList = ['2 Weeks: Time 0', '2 Weeks: Time 30', '2 Weeks: Time 60', '2 Weeks: Time 90', '2 Weeks: Time 120', \n",
    "               '2 Weeks: Time 150', '4 Weeks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get clinic data features\n",
    "def ClinicDataAggregator(subjDict,taskList_Abb,taskScores,sessionList):\n",
    "    \n",
    "    # compute features for each subject-task pairing\n",
    "    Data = pd.DataFrame()\n",
    "    numSamples = pd.DataFrame()\n",
    "    s = 0\n",
    "    for subj in list(subjDict.keys()):\n",
    "        s += 1\n",
    "        print('Subject %d (%d of %d)'%(subj,s,len(subjDict)))\n",
    "        for t in range(len(taskList_Abb)):\n",
    "            task = taskList_Abb[t]\n",
    "            task_score = taskScores[t]\n",
    "            for trial in range(len(sessionList)):\n",
    "                visit = sessionList[trial]\n",
    "                \n",
    "                # get task ACC data\n",
    "                try:\n",
    "                    data = pd.read_csv('//FS2.smpp.local//RTO//CIS-PD Study//MJFF Curation//TaskAcc//' + str(subj) + '_' + str(trial) + \n",
    "                                      '_' + task + '.csv',parse_dates=['timestamp'])[['timestamp','x','y','z']]\n",
    "                except:\n",
    "                    print('No data found for %s trial %d'%(task,trial))\n",
    "                    continue\n",
    "                    \n",
    "                # get Tremor and Bradykinesia Scores\n",
    "                side = subjDict[subj]\n",
    "                subj_score = mot_scores.loc[mot_scores['subject']==subj,['subject','visit',\n",
    "                                            task_score+ ' ' + 'bradykinesia ' + side + ' upper limb',\n",
    "                                            task_score+ ' ' + 'tremor ' + side + ' upper limb']]\n",
    "                subj_score = subj_score.rename(index=str,columns={subj_score.columns[2]:'Bradykinesia',subj_score.columns[3]:'Tremor'})\n",
    "                subj_score.index = range(len(subj_score))\n",
    "                    \n",
    "                # reset time starting at first data point\n",
    "                data['timestamp'] = (data.timestamp.values - data.timestamp.values[0]).astype('timedelta64[ms]').astype(int)\n",
    "                data = data.set_index('timestamp')\n",
    "                \n",
    "                # filter data\n",
    "                data = filterdata(data,'highpass')\n",
    "                data = filterdata(data,'lowpass',cutoff=3)\n",
    "                \n",
    "                # get data clips\n",
    "                clip_data = gen_clips(data)\n",
    "                \n",
    "                # get the acc features\n",
    "                for c in range(len(clip_data['data'])):\n",
    "                    N = pd.DataFrame()\n",
    "                    N['Subject'] = subj\n",
    "                    N['Task'] = task\n",
    "                    N['Visit'] = sessionList[trial]\n",
    "                    N['Samples'] = len(clip_data['data'][c])\n",
    "                    numSamples = pd.concat([numSamples,N])\n",
    "                feature_extraction_reduced(clip_data)\n",
    "                if 'features' in clip_data.keys():\n",
    "                    D = clip_data['features']\n",
    "                    featcols = list(D.columns)\n",
    "                    D['Bradykinesia'] = subj_score['Bradykinesia'][trial]\n",
    "                    D['Tremor'] = subj_score['Tremor'][trial]\n",
    "                    D['Visit'] = visit\n",
    "                    D['Task'] = task\n",
    "                    D['Subject'] = subj\n",
    "                    Data = pd.concat([Data,D])\n",
    "    # save acc features      \n",
    "    cols = ['Subject','Visit','Task','Bradykinesia','Tremor'] + featcols\n",
    "    Data = Data[cols]\n",
    "    Data.to_csv('//FS2.smpp.local/RTO/CIS-PD Study\\\\Clinic WACC features\\\\REPLACE FILE NAME.csv')\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Files:\n",
    "files = os.listdir(\"//FS2.smpp.local\\\\RTO\\\\CIS-PD Study\\\\MJFF Curation\\\\ClinicVisitACC\")\n",
    "\n",
    "for file in files:\n",
    "    if os.path.isfile('X:\\CIS-PD Study\\Home WACC features\\\\features ' + file):\n",
    "        continue\n",
    "    Data = HomeDataAggregator(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Features on home data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 4 digit user_id from the corresponding 6 digit user_id\n",
    "def user_id_6_to_4(user_id_6):\n",
    "    for index, row in user_id_pairings.iterrows():\n",
    "        if (not np.isnan(row['Subj ID Athena'])):\n",
    "            if (int(row['Subj ID Athena'])==int(user_id_6)):\n",
    "                return int(row['SubjectCode'])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all home data features - less efficient version\n",
    "def HomeDataAggregator(file):\n",
    "    t1 = time.time()\n",
    "    Data = pd.DataFrame()\n",
    "    print(file)\n",
    "    \n",
    "    features_list = ['RMSX','RMSY','RMSZ','rangeX','rangeY','rangeZ','meanX','meanY','meanZ','varX','varY','varZ',\n",
    "                    'skewX','skewY','skewZ','kurtX','kurtY','kurtZ','xcor_peakXY','xcorr_peakXZ','xcorr_peakYZ',\n",
    "                    'xcorr_lagXY','xcorr_lagXZ','xcorr_lagYZ','Dom_freq','Pdom_rel','PSD_mean','PSD_std','PSD_skew',\n",
    "                    'PSD_kur','jerk_mean','jerk_std','jerk_skew','jerk_kur']\n",
    "        \n",
    "    # get acc data\n",
    "    try:\n",
    "        # change if we want to use Activity Level and Tremor Score\n",
    "        data = pd.read_pickle('X:\\\\CIS-PD Study\\\\MJFF Curation\\\\combined pre-visit data\\\\' + \n",
    "                               file)[['user_id','timestamp','Gait','x','y','z']]  \n",
    "    except:\n",
    "        print('No data found for %s trial %d'%(task,trial))\n",
    "        return\n",
    "        \n",
    "    # organize data and make 5 second clips\n",
    "    data = data.sort_values(by = 'timestamp', axis = 0)\n",
    "    data['timestamp2'] = [(tm - datetime.timedelta(minutes=0,\n",
    "                                                   seconds=tm.second % 5,\n",
    "                                                   microseconds=tm.microsecond)) \n",
    "                          for tm in data.timestamp]\n",
    "    \n",
    "    data['timestamp'] = (data.timestamp.values - data.timestamp.values[0]).astype('timedelta64[ms]').astype(int)\n",
    "    data = data.set_index('timestamp')\n",
    "    data.loc[:,['x', 'y', 'z']] = filterdata(data[['x', 'y', 'z']])\n",
    "                \n",
    "    # \"clip\" the data into 5 second chunks    \n",
    "    five_sec_intervals = data.timestamp2.unique()\n",
    "        \n",
    "    # calculate features\n",
    "    F=[]\n",
    "    num_empty = 0\n",
    "    times = []\n",
    "    for t in five_sec_intervals:\n",
    "        clip = data.loc[(data.timestamp2 == t)] # & (data.Gait == 1)] # take only walking data\n",
    "        if (clip.empty or (len(clip.timestamp2) < 200)):\n",
    "            num_empty += 1\n",
    "        else:\n",
    "            F.append(reduced_feature_extraction_from_1_clip(clip[['x', 'y', 'z']]))\n",
    "            times.append(t)\n",
    "\n",
    "    success_info = [file, len(five_sec_intervals), (len(five_sec_intervals)-num_empty)]\n",
    "    df = pd.DataFrame(data = [success_info], columns = ['file', 'num expected clips', 'num actual clips'])\n",
    "    if (os.path.isfile(\"X:\\\\CIS-PD Study\\\\Home WACC features\\\\success info.csv\")):\n",
    "            dfo = pd.read_csv(\"X:\\\\CIS-PD Study\\\\Home WACC features\\\\success info.csv\", index_col = 0)\n",
    "            df = pd.concat([dfo, df])\n",
    "    df.to_csv(\"X:\\\\CIS-PD Study\\\\Home WACC features\\\\success info.csv\")\n",
    "            \n",
    "    # create features dataframe\n",
    "    D = pd.DataFrame(data=F,columns=features_list,dtype='float32')    \n",
    "    featcols = list(D.columns)\n",
    "    D['Subject'] = data.loc[0, 'user_id']\n",
    "    D['timestamp'] = times #['timestamp2']\n",
    "    Data = pd.concat([Data,D])   \n",
    "    cols = ['Subject','timestamp'] + featcols\n",
    "    Data = Data[cols]\n",
    "    elapsed_time = ((time.time() - t1)/60.0).__str__()\n",
    "    print(elapsed_time + \" mins\\n\")\n",
    "    Data.to_pickle('X:\\CIS-PD Study\\Home WACC features\\\\features ' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features using threading - helper function\n",
    "def mapping_func(t, data):\n",
    "    clip = data.loc[(data.timestamp2 == t)] # & (data.Gait == 1)] # take only walking data\n",
    "    if not((clip.empty or (len(clip.index) < 200))):\n",
    "        return list(reduced_feature_extraction_from_1_clip(clip[['x', 'y', 'z']])).append(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get home features using threading\n",
    "def HomeDataAggregatorThreading(file):\n",
    "    t1 = time.time()\n",
    "    Data = pd.DataFrame()\n",
    "    print(file)\n",
    "    \n",
    "    features_list = ['RMSX','RMSY','RMSZ','rangeX','rangeY','rangeZ','meanX','meanY','meanZ','varX','varY','varZ',\n",
    "                    'skewX','skewY','skewZ','kurtX','kurtY','kurtZ','xcor_peakXY','xcorr_peakXZ','xcorr_peakYZ',\n",
    "                    'xcorr_lagXY','xcorr_lagXZ','xcorr_lagYZ','Dom_freq','Pdom_rel','PSD_mean','PSD_std','PSD_skew',\n",
    "                    'PSD_kur','jerk_mean','jerk_std','jerk_skew','jerk_kur', 'timestamp']\n",
    "        \n",
    "    # get acc data\n",
    "    try:\n",
    "        # change if we want to use Activity Level and Tremor Score\n",
    "        data = pd.read_pickle('X:\\\\CIS-PD Study\\\\MJFF Curation\\\\combined pre-visit data\\\\' + \n",
    "                               file)[['user_id','timestamp','Gait','x','y','z']]  \n",
    "    except:\n",
    "        print('No data found for %s trial %d'%(task,trial))\n",
    "        return\n",
    "        \n",
    "    # organize data and make 5 second clips\n",
    "    data = data.sort_values(by = 'timestamp', axis = 0)\n",
    "    \n",
    "    data['timestamp2'] = [(tm - datetime.timedelta(minutes=0,\n",
    "                                                   seconds=tm.second % 5,\n",
    "                                                   microseconds=tm.microsecond)) \n",
    "                          for tm in data.timestamp]\n",
    "    \n",
    "    data['timestamp'] = (data.timestamp.values - data.timestamp.values[0]).astype('timedelta64[ms]').astype(int)\n",
    "    data = data.set_index('timestamp')\n",
    "    data.loc[:,['x', 'y', 'z']] = filterdata(data[['x', 'y', 'z']])\n",
    "                \n",
    "    # \"clip\" the data into 5 second chunks    \n",
    "    five_sec_intervals = data.timestamp2.unique()\n",
    "        \n",
    "    # calculate features\n",
    "    pool = ThreadPool(4)\n",
    "    results = pool.map(lambda t : mapping_func(t, data), five_sec_intervals)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(\"\\n\\n\")\n",
    "    print(results)\n",
    "    results = list(results)\n",
    "    print('\\n')\n",
    "    print (results)\n",
    "    results = list(filter(lambda r: r is not None, results))\n",
    "    success_info = [file, len(five_sec_intervals), len(results)]\n",
    "    df = pd.DataFrame(data = [success_info], columns = ['file', 'num expected clips', 'num actual clips'])\n",
    "    if (os.path.isfile(\"X:\\\\CIS-PD Study\\\\Home WACC features\\\\success info.csv\")):\n",
    "            dfo = pd.read_csv(\"X:\\\\CIS-PD Study\\\\Home WACC features\\\\success info.csv\", index_col = 0)\n",
    "            df = pd.concat([dfo, df])\n",
    "    df.to_csv(\"X:\\\\CIS-PD Study\\\\Home WACC features\\\\success info.csv\")\n",
    "            \n",
    "    # create features dataframe\n",
    "    D = pd.DataFrame(data=results,columns=features_list,dtype='float32')    \n",
    "    featcols = list(D.columns)\n",
    "    D['Subject'] = data.loc[0, 'user_id']\n",
    "    Data = pd.concat([Data,D])   \n",
    "    cols = ['Subject'] + featcols\n",
    "    Data = Data[cols]\n",
    "    elapsed_time = ((time.time() - t1)/60.0).__str__()\n",
    "    print(elapsed_time + \" mins\\n\")\n",
    "    Data.to_pickle('X:\\CIS-PD Study\\Home WACC features\\\\features ' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142609 2017-09-01.pkl\n",
      "22.153518044948576 mins\n",
      "\n",
      "142578 2017-09-05.pkl\n",
      "309.9703974843025 mins\n",
      "\n",
      "142616 2017-07-19.pkl\n",
      "587.6584043860436 mins\n",
      "\n",
      "142593 2017-11-06.pkl\n",
      "6.090699501832327 mins\n",
      "\n",
      "142600 2017-07-25.pkl\n",
      "14.770508551597596 mins\n",
      "\n",
      "142606 2017-08-16.pkl\n",
      "2.7018452882766724 mins\n",
      "\n",
      "142616 2017-08-01.pkl\n",
      "4.589166724681855 mins\n",
      "\n",
      "142615 2017-07-07.pkl\n",
      "19.4826144973437 mins\n",
      "\n",
      "142595 2017-08-22.pkl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b29c1eca2db9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'X:\\CIS-PD Study\\Home WACC features\\\\features '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHomeDataAggregator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-6c81cee018cd>\u001b[0m in \u001b[0;36mHomeDataAggregator\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mtimes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfive_sec_intervals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mclip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestamp2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# & (data.Gait == 1)] # take only walking data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestamp2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mnum_empty\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other, axis)\u001b[0m\n\u001b[0;32m   1199\u001b[0m             \u001b[1;31m# Series/Index behavior\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m             res_values = dispatch_to_index_op(op, self, other,\n\u001b[1;32m-> 1201\u001b[1;33m                                               pd.DatetimeIndex)\n\u001b[0m\u001b[0;32m   1202\u001b[0m             return self._constructor(res_values, index=self.index,\n\u001b[0;32m   1203\u001b[0m                                      name=res_name)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mdispatch_to_index_op\u001b[1;34m(op, left, right, index_class)\u001b[0m\n\u001b[0;32m   1096\u001b[0m         \u001b[0mleft_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleft_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shallow_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mNullFrequencyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m         \u001b[1;31m# DatetimeIndex and TimedeltaIndex with freq == None raise ValueError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\datetimes.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_tzawareness_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mcmp_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m# representations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mneeds_i8_conversion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mneeds_i8_conversion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate_compare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\datetimelike.py\u001b[0m in \u001b[0;36m_evaluate_compare\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;31m# technically we could support bool dtyped Index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;31m# for now just return the indexing array directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_isnan\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_isnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_bool_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# All Files:\n",
    "#files = os.listdir(\"X:\\\\CIS-PD Study\\\\MJFF Curation\\\\combined pre-visit data\")\n",
    "\n",
    "# Luca's Files:\n",
    "# files = ['142605 2017-08-16.pkl', '142557 2017-09-21.pkl', '142559 2017-07-06.pkl', '142617 2017-07-17.pkl', '142582 2017-07-27.pkl', '142577 2017-11-30.pkl', '142604 2017-10-02.pkl', '142604 2017-07-21.pkl', '142568 2017-09-18.pkl', '142615 2017-07-21.pkl', '142570 2017-07-13.pkl', '142623 2017-10-12.pkl', '142559 2017-07-20.pkl', '142602 2017-12-12.pkl', '142592 2017-10-03.pkl', '142561 2017-07-03.pkl', '142584 2018-02-21.pkl', '142562 2017-07-07.pkl', '142622 2017-08-03.pkl', '142606 2017-10-13.pkl', '142604 2017-08-14.pkl', '142568 2018-02-28.pkl', '142583 2017-10-13.pkl', '142618 2017-07-31.pkl', '142583 2017-08-25.pkl', '142603 2017-08-07.pkl', '142615 2017-10-05.pkl', '142595 2017-10-12.pkl', '142593 2018-01-29.pkl', '142623 2017-08-17.pkl', '142601 2017-07-13.pkl', '142570 2017-08-03.pkl', '142568 2017-09-06.pkl', '142602 2017-08-03.pkl', '142618 2017-10-30.pkl', '142616 2018-02-05.pkl', '142584 2017-11-29.pkl', '142598 2017-07-24.pkl', '142608 2017-09-18.pkl']\n",
    "\n",
    "# Nick's Files:\n",
    "# files = ['142598 2017-07-10.pkl', '142617 2017-08-01.pkl', '142603 2017-07-31.pkl', '142605 2017-08-02.pkl', '142618 2018-01-30.pkl', '142592 2017-07-13.pkl', '142558 2017-06-29.pkl', '142620 2017-07-21.pkl', '142593 2017-07-20.pkl', '142617 2017-10-03.pkl', '142594 2017-08-10.pkl', '142592 2017-07-27.pkl', '142578 2018-02-19.pkl', '142621 2017-07-21.pkl', '142612 2017-09-06.pkl', '142609 2017-08-11.pkl', '142612 2017-08-15.pkl', '142619 2017-08-10.pkl', '142608 2017-09-11.pkl', '142558 2017-07-14.pkl', '142585 2017-10-04.pkl', '142595 2018-01-10.pkl', '142557 2017-06-29.pkl', '142622 2017-10-12.pkl', '142601 2017-07-26.pkl', '142575 2017-08-29.pkl', '142581 2017-07-26.pkl', '142566 2017-07-26.pkl', '142608 2017-11-09.pkl', '142602 2017-09-20.pkl', '142606 2017-12-29.pkl', '142567 2017-07-31.pkl', '142620 2017-08-10.pkl', '142622 2017-08-17.pkl', '142583 2017-08-09.pkl', '142602 2017-07-13.pkl', '142566 2017-08-09.pkl', '142580 2017-08-01.pkl']\n",
    "\n",
    "# Julianne's Files:\n",
    "files = ['142618 2017-08-14.pkl', '142557 2017-07-13.pkl', '142585 2018-02-02.pkl', '142581 2017-08-14.pkl', '142577 2017-08-03.pkl', '142619 2018-01-22.pkl', '142584 2017-11-01.pkl', '142600 2017-07-11.pkl', '142620 2017-10-30.pkl', '142593 2017-08-03.pkl', '142621 2017-10-23.pkl', '142609 2017-09-01.pkl', '142578 2017-09-05.pkl', '142616 2017-07-19.pkl', '142593 2017-11-06.pkl', '142600 2017-07-25.pkl', '142606 2017-08-16.pkl', '142616 2017-08-01.pkl', '142615 2017-07-07.pkl', '142595 2017-08-22.pkl', '142615 2018-01-22.pkl', '142598 2017-09-13.pkl', '142557 2017-12-11.pkl', '142601 2017-09-20.pkl', '142582 2017-08-09.pkl', '142594 2017-07-19.pkl', '142577 2017-10-11.pkl', '142579 2017-07-18.pkl', '142619 2017-07-21.pkl', '142578 2017-09-18.pkl', '142605 2017-10-11.pkl', '142575 2017-08-16.pkl', '142580 2017-07-14.pkl', '142623 2017-08-03.pkl', '142563 2017-07-18.pkl', '142604 2017-11-27.pkl', '142584 2017-09-13.pkl', '142616 2017-10-03.pkl', '142560 2017-07-03.pkl']\n",
    "\n",
    "for file in files:\n",
    "    if os.path.isfile('X:\\CIS-PD Study\\Home WACC features\\\\features ' + file):\n",
    "        continue\n",
    "    Data = HomeDataAggregator(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pool = ThreadPool(2)\n",
    "#pool.map(HomeDataAggregator, files)\n",
    "#pool.close()\n",
    "#pool.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
